str_extract(regex("(?<=:\\s)(.*)$"))
export_date %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
sections
all_sentences <- sections[[1]][4]
text_parts <- all_sentences %>%
str_match_all(regex("(.*?)<B>(.+?)</>(.*?)\\(((?:A09|A97)/.*?)\\)\\s*\\n", dotall = TRUE))
unique_tokens <- unique(text_parts[[1]][,3])
unique_tokens
sources <- text_parts[[1]][,5]
sources
sources <- text_parts[[1]][,5]
# Unique tokens in the sentences
data <- data.frame(Tokens = unique(text_parts[[1]][,3]))
data
# Unique tokens in the sentences
data <- data.frame(Tokens = text_parts[[1]][,3])
data
# Source information
data$Sources <- text_parts[[1]][,5]
data
data$ExportDate <- export_date
raw.file.2 <- scan("test6.TXT", sep="\n", what=character(), fileEncoding="latin1")
raw.file <- read_file("test6.TXT", locale(encoding="latin1"))
# Metadata ----------------------------------------------------------------
# Save COSMAS version
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Split file into sections
sections <- raw.file %>%
str_split("\\_{80}")
# Save export date
export_date <- sections[[1]][2]
export_date %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# # Save the search phrase
# Phrase <- raw.file %>%
#             str_extract(regex("(?<=^Suchanfrage)(.*)$")) %>%
#             str_extract(regex("(?<=:\\s)(.*)$")) %>%
#             str_subset(regex(".*"))
#
# # Split the file into sections based on underscores
# sections2 <- raw.file %>%
# str_which(regex("^\\_{80}", multiline = TRUE))
# Sentences and their information -----------------------------------------
all_sentences <- sections[[1]][4]
text_parts <- all_sentences %>%
str_match_all(regex("(.*?)<B>(.+?)</>(.*?)\\(((?:A09|A97)/.*?)\\)\\s*\\n", dotall = TRUE))
# Unique tokens in the sentences
data <- data.frame(Tokens = text_parts[[1]][,3])
# Source information
data$Sources <- text_parts[[1]][,5]
data$Export_Date <- export_date
data$C2API_Version <- C2API_Version
data
export_date <- sections[[1]][2]
export_date %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
export_date <- export_date %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
data$Export_Date <- export_date
data
text_parts[[1]][,2]
data <- data.frame()
# Metadata ----------------------------------------------------------------
# Save COSMAS version
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Metadata ----------------------------------------------------------------
# Save COSMAS version
data$C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
data$export_date <- export_date
sections[[1]]
sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Save export date
export_date <- sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
export_date
# # Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)"))
sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)"))
sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# # Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
text_parts[[1]][,2]
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Split file into sections
sections <- raw.file %>%
str_split("\\_{80}")
# Save export date
export_date <- sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# # Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Sentences and their information -----------------------------------------
all_sentences <- sections[[1]][4]
text_parts <- all_sentences %>%
str_match_all(regex("(.*?)<B>(.+?)</>(.*?)\\(((?:A09|A97)/.*?)\\)\\s*\\n",
dotall = TRUE))
# Unique tokens in the sentences
data <- data.frame(Tokens = text_parts[[1]][,3])
# Source information
data$Sources <- text_parts[[1]][,5]
# Creating data frame for export ------------------------------------------
data$Export_Date <- export_date
data$C2API_Version <- C2API_Version
shiny::runApp('CorpusReader')
?hr()
install.packages(c("wordcloud", "tm"))
library(wordcloud)
library(tm)
all_sentences %>%
removePunctuation()
all_sentences %>%
removePunctuation() %>%
removeNumbers()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
select(text)
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
TermDocumentMatrix()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
content_transformer(tolower)
?content_transformer
??tolower
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower()
?removeWords
data("crude")
crude[[1]]
removeWords(crude[[1]], stopwords("english"))
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
TermDocumentMatrix()
#Create a vector containing only the text
text <- data$text# Create a corpus
docs <- Corpus(VectorSource(text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
docs
dtm
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
count()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split()
?str_split()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ")
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
count()
?count
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
tally()
foo <-
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
count()
foo <-
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ")
View(foo)
foo[1]
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist()
foo <-
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist() %>%
count()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist() %>%
length()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist() %>%
TermDocumentMatrix()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
# unlist() %>%
TermDocumentMatrix()
?TermDocumentMatrix()
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist() %>%
count()
?count()
starwars %>% count(species)
foo <- as.data.frame(all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist())
foo
foo %>% count()
head(foo)
foo <- as.data.frame(words =
all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist())
foo <- all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist())
foo <- all_sentences %>%
removePunctuation() %>%
removeNumbers() %>%
stripWhitespace() %>%
str_to_lower() %>%
str_split(" ") %>%
unlist()
foo <- as.data.frame(words = foo)
foo[1]
foo <- as.data.frame(foo)
foo
library(tidyr)
library(stringr)
library(dplyr)
library(readr)
library(purrr)
# Reading in the raw COSMAS file
# FIXME  add encoding options "latin1" and "UTF-8"
raw.file <- read_file("test.txt", locale(encoding="latin1"))
# Metadata ----------------------------------------------------------------
# Save COSMAS version
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Split file into sections
sections <- raw.file %>%
str_split("\\_{80}")
# Save export date
Export_Date <- sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Sentences and their information -----------------------------------------
corpora <- sections[[1]][3]
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique()
corporaID
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
paste(sep = "|")
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
paste(sep = "|")
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique()
corporaID
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
str_c(sep="|", collapse = "")
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(sep="|", collapse = "")
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(sep="|")
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(sep="|", collapse = "|")
corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(collapse = "|")
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(collapse = "|")
all_sentences <- sections[[1]][4]
# corpora <- "A09|A97"
text_parts <- all_sentences %>%
str_match_all(regex(paste("(.*?)<B>(.+?)</>(.*?)\\(((?:",corporaID,")/.*?)\\)\\s*\\n", sep=""),
dotall = TRUE))
text_parts
regex(paste("(.*?)<B>(.+?)</>(.*?)\\(((?:",corporaID,")/.*?)\\)\\s*\\n", sep=""),
dotall = TRUE)
# Source information
data <- data.frame(Sources = text_parts[[1]][,5])
# Tokens
data$Token <- text_parts[[1]][,3] %>%
str_trim()
# Context sentence BEFORE token sentence
data$Precontext <- text_parts[[1]][,2] %>%
str_extract_all(boundary("sentence")) %>%
map(function(x) {nth(x,-2)} ) %>%
str_trim() %>%
unlist()
# Sentence part BEFORE token
data$Prehit <- text_parts[[1]][,2] %>%
str_extract_all(boundary("sentence")) %>%
map(last) %>%
str_trim() %>%
unlist()
# Sentence part AFTER token
data$Posthit <- text_parts[[1]][,4] %>%
str_extract_all(boundary("sentence")) %>%
map(first) %>%
str_trim() %>%
unlist()
# Extract context sentence AFTER token sentence
data$Postcontext <- text_parts[[1]][,4] %>%
str_extract_all(boundary("sentence")) %>%
map(function(x) {nth(x,2)} ) %>%
str_trim() %>%
unlist()
# Creating data frame for export ------------------------------------------
data <-
data %>%
unite(Prehit, Token, Posthit, col="Sentence", sep = " ", remove=F) %>%
mutate(C2API_Version = C2API_Version, Export_Date = Export_Date) %>%
select(C2API_Version, Export_Date, Token, Precontext, Sentence, Postcontext) %>%
replace_na(list(Precontext = "", Postcontext = ""))
data
head(data)
shiny::runApp('COSMAS2-Reader')
data$Token
runApp('COSMAS2-Reader')
unique_tokens <- unique(data$Token)
unique_tokens
print(unique_tokens)
runApp('COSMAS2-Reader')
