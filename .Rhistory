geom_errorbar(aes(ymax=MRT+sem6,ymin=MRT-sem6),
width=.1,size=.5, show.legend=FALSE) +
min
ggsave(filename="ro.png",units="cm",width=13,height=13,dpi=600)
ggsave(filename="rt6.png",units="cm",width=13,height=13,dpi=600)
ggplot(dat6,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial),size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(uniblue,black)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(1,2)) +
scale_x_discrete("Directional phrase",
labels=c("ambiguous", "telic")) +
ylab("Mean RT in (ms)") +
coord_cartesian(ylim=c(1000,1300)) +
scale_y_continuous(breaks=seq(0, 5000, 50)) +                                       #Ticks from 0-10, every .25
xlab("Directional phrase") +
geom_errorbar(aes(ymax=MRT+sem6,ymin=MRT-sem6),
width=.1,size=.5, show.legend=FALSE) +
min
ggsave(filename="rt6.png",units="cm",width=13,height=12,dpi=600)
ggplot(dat6,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial),size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,uniblue)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(2,1)) +
scale_x_discrete("Directional phrase",
labels=c("ambiguous", "telic")) +
ylab("Mean RT in (ms)") +
coord_cartesian(ylim=c(1000,1300)) +
scale_y_continuous(breaks=seq(0, 5000, 50)) +                                       #Ticks from 0-10, every .25
xlab("Directional phrase") +
geom_errorbar(aes(ymax=MRT+sem6,ymin=MRT-sem6),
width=.1,size=.5, show.legend=FALSE) +
min
ggsave(filename="rt6.png",units="cm",width=13,height=12,dpi=600)
ggplot(aj3,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial), size=0.5) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,unigray)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(1,3)) +
coord_cartesian(ylim=c(3.75,4.75)) +                                                      #Coordinates (min, max)
scale_x_discrete("Directional phrase",
labels=c(expression(paste("ambiguous (", alpha, ")")), "telic")) +
ylab("Mean acceptability rating") +
geom_errorbar(aes(ymax=MRT+sem,ymin=MRT-sem),
width=.1,size=.5, show.legend=FALSE) +                   #Error bars
min
ggplot(aj3,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial), size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,unigray)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(1,3)) +
coord_cartesian(ylim=c(3.75,4.75)) +                                                      #Coordinates (min, max)
scale_x_discrete("Directional phrase",
labels=c("ambiguous","telic")) +
ylab("Mean acceptability rating") +
geom_errorbar(aes(ymax=MRT+sem,ymin=MRT-sem),
width=.1,size=.5, show.legend=FALSE) +                   #Error bars
min
ggplot(dat6,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial),size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,uniblue)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(2,1)) +
scale_x_discrete("Directional phrase",
labels=c("ambiguous", "telic")) +
ylab("Mean RT (ms)") +
coord_cartesian(ylim=c(1000,1300)) +
scale_y_continuous(breaks=seq(0, 5000, 50)) +                                       #Ticks from 0-10, every .25
xlab("Directional phrase") +
geom_errorbar(aes(ymax=MRT+sem6,ymin=MRT-sem6),
size=0.5, width=0.1, show.legend=FALSE) +
min
ggsave(filename="rt6.png",units="cm",width=13,height=11,dpi=600)
ggplot(aj3,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial), size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,unigray)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(1,3)) +
coord_cartesian(ylim=c(3.75,4.75)) +                                                      #Coordinates (min, max)
scale_x_discrete("Directional phrase",
labels=c("ambiguous","telic")) +
ylab("Mean acceptability rating") +
geom_errorbar(aes(ymax=MRT+sem,ymin=MRT-sem),
size=0.5, width=0.1, show.legend=FALSE) +                   #Error bars
min
ggplot(dat6,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial),size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,uniblue)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(2,1)) +
scale_x_discrete("Directional phrase",
labels=c("ambiguous", "telic")) +
ylab("Mean RT (ms)") +
coord_cartesian(ylim=c(1000,1300)) +
scale_y_continuous(breaks=seq(0, 5000, 50)) +                                       #Ticks from 0-10, every .25
xlab("Directional phrase") +
geom_errorbar(aes(ymax=MRT+sem6,ymin=MRT-sem6),
size=0.5, width=0.1, show.legend=FALSE) +
min
ggsave(filename="rt6.png",units="cm",width=13,height=11,dpi=600)
ggplot(aj3,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial), size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,uniblue)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(2,1)) +
coord_cartesian(ylim=c(3.75,4.75)) +                                                      #Coordinates (min, max)
scale_x_discrete("Directional phrase",
labels=c("ambiguous","telic")) +
ylab("Mean acceptability rating") +
geom_errorbar(aes(ymax=MRT+sem,ymin=MRT-sem),
size=0.5, width=0.1, show.legend=FALSE) +                   #Error bars
min
ggsave(filename="aj.png",units="cm",width=13,height=11,dpi=600)
ggplot(aj3,
aes(colour=Temporaladverbial,y=MRT, x=Direktionalphrase)) +
geom_line(aes(linetype=Temporaladverbial, group=Temporaladverbial), size=1) +
scale_color_manual(name="Temporal\n adverbial",
values=c(black,uniblue)) +
scale_linetype_manual(name="Temporal\n adverbial",
values=c(2,1)) +
coord_cartesian(ylim=c(3.75,4.75)) +                                                      #Coordinates (min, max)
scale_x_discrete("Directional phrase",
labels=c("ambiguous","telic")) +
ylab("Mean rating (1-5)") +
geom_errorbar(aes(ymax=MRT+sem,ymin=MRT-sem),
size=0.5, width=0.1, show.legend=FALSE) +                   #Error bars
min
ggsave(filename="aj.png",units="cm",width=13,height=11,dpi=600)
install.packages(c("cowplot", "googleway", "ggplot2", "ggrepel",
)
)
install.packages(c("cowplot", "googleway", "ggplot2", "ggrepel",
)
install.packages(c("backports", "bayestestR", "broom", "dbplyr", "dplyr", "effectsize", "ellipsis", "emmeans", "fs", "ggplot2", "ggpubr", "haven", "htmlTable", "htmltools", "insight", "isoband", "jsonlite", "lme4", "lmerTest", "lubridate", "modelr", "mvtnorm", "nloptr", "parameters", "performance", "pillar", "pkgbuild", "pkgload", "processx", "ps", "purrr", "Rcpp", "reshape2", "rlang", "rmarkdown", "scales", "see", "tibble", "tidyr", "tidyselect", "tinytex", "vctrs", "withr"))
install.packages(c("backports", "bayestestR", "broom", "dbplyr", "dplyr", "effectsize", "ellipsis", "emmeans", "fs", "ggplot2", "ggpubr", "haven", "htmlTable", "htmltools", "insight", "isoband", "jsonlite", "lme4", "lmerTest", "lubridate", "modelr", "mvtnorm", "nloptr", "parameters", "performance", "pillar", "pkgbuild", "pkgload", "processx", "ps", "purrr", "Rcpp", "reshape2", "rlang", "rmarkdown", "scales", "see", "tibble", "tidyr", "tidyselect", "tinytex", "vctrs", "withr"))
install.packages(c("backports", "bayestestR", "broom", "dbplyr", "dplyr", "effectsize", "ellipsis", "emmeans", "fs", "ggplot2", "ggpubr", "haven", "htmlTable", "htmltools", "insight", "isoband", "jsonlite", "lme4", "lmerTest", "lubridate", "modelr", "mvtnorm", "nloptr", "parameters", "performance", "pillar", "pkgbuild", "pkgload", "processx", "ps", "purrr", "Rcpp", "reshape2", "rlang", "rmarkdown", "scales", "see", "tibble", "tidyr", "tidyselect", "tinytex", "vctrs", "withr"))
install.packages(c("cowplot", "googleway"))
install.packages(c("cowplot", "googleway", "ggrepel", "ggspatial", "sf", "rnaturalearth", "rnaturalearthdata"))
library("ggplot2")
theme_set(theme_bw())
library("sf")
install.packages("sf")
world <- ne_countries(scale = "medium", returnclass = "sf")
install.packages(c("cowplot", "googleway", "ggplot2", "ggrepel",
"ggspatial", "libwgeom", "sf", "rnaturalearth",
"rnaturalearthdata"))
library("ggplot2")
theme_set(theme_bw())
library("sf")
library("rnaturalearth")
library("rnaturalearthdata")
world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)
install.packages("sf")
install.packages("units")
install.packages("~/Downloads/units_0.6-7.tar.gz", repos = NULL, type = "source")
install.packages("libudunits2")
library(sf)
library(raster)
library(dplyr)
library(spData)
library(spDataLarge)
install.packages(c("sf", "raster", "spData", "spDataLarge"))
install.packages(c(""cowplot"", ""googleway"", ""ggplot2"", ""ggrepel"", ""ggspatial"", ""libwgeom"", ""sf"", ""rnaturalearth"", ""rnaturalearthdata""))
install.packages(c("cowplot", "googleway", "ggrepel", "ggspatial", "libwgeom", "sf", "rnaturalearth", "rnaturalearthdata"))
install.packages("httr")
install.packages("openssl")
install.packages(c("cowplot", "googleway", "ggplot2", "ggrepel", "ggspatial", "libwgeom", "sf", "rnaturalearth", "rnaturalearthdata"))
install.packages("~/Downloads/units_0.6-7.tar.gz", repos = NULL, type = "source")
install.packages(c("broom", "esquisse", "ggforce", "ggsignif", "qqplotr", "see"))
install.packages("tidyr")
shiny::runApp('Documents/Work/A1/Corpus/CorpusReader')
install.packages("bslib")
remove.packages("shiny", lib="~/R/x86_64-redhat-linux-gnu-library/4.0")
install.packages("shiny")
install.packages("bslib")
install.packages("/tmp/RtmpsP2grN/downloaded_packages/bslib_0.2.5.1.tar.gz", repos = NULL, type = "source")
install.packages("/tmp/RtmpsP2grN/downloaded_packages/sass_0.4.0.tar.gz", repos = NULL, type = "source")
library(shiny)
install.packages("shiny")
library(shiny)
library(here)
belege <- scan(file.choose(), sep="\n", what=character(), fileEncoding="latin1")
runApp('Documents/Work/A1/Corpus/CorpusReader')
runApp('Documents/Work/A1/Corpus/CorpusReader')
?fileInput
runApp('Documents/Work/A1/Corpus/CorpusReader')
?reg
??reg
runApp('Documents/Work/A1/Corpus/CorpusReader')
runApp()
runApp('Documents/Work/A1/Corpus/CorpusReader')
runApp()
runApp('Documents/Work/A1/Corpus/CorpusReader')
runApp()
runApp('Documents/Work/A1/Corpus/CorpusReader')
shiny::runApp()
ui <- fluidPage(
# Application title
titlePanel("COSMAS II Exportdatei in Tabelle umwandeln"),
sidebarLayout(
sidebarPanel(
# Input raw COSMAS CSV-file
fileInput(inputId = "belege",
label = "CSV-Datei einlesen",
buttonLabel = "Durchsuchen",
placeholder = "Noch keine Datei",
accept = ".csv"),
# Choose encoding
radioButtons("encoding", "Codierung:",
c("Latin1" = "latin1",
"UTF-8" = "utf-8")),
# checkboxInput("header", "Header", TRUE),
downloadButton("downloadData", "Download")
),
mainPanel(
tableOutput("contents")
)
)
)
# Frontend
ui <- fluidPage(
# Application title
titlePanel("COSMAS II Exportdatei in Tabelle umwandeln"),
sidebarLayout(
sidebarPanel(
# Input raw COSMAS CSV-file
fileInput(inputId = "belege",
label = "CSV-Datei einlesen",
buttonLabel = "Durchsuchen",
placeholder = "Noch keine Datei",
accept = ".csv"),
# Choose encoding
radioButtons("encoding", "Codierung:",
c("Latin1" = "latin1",
"UTF-8" = "utf-8")),
# checkboxInput("header", "Header", TRUE),
downloadButton("downloadData", "Download")
),
mainPanel(
tableOutput("contents")
)
)
)
titlePanel("COSMAS II Exportdatei in Tabelle umwandeln")
shiny::runApp()
shiny::runApp()
?scan
runApp()
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(here)
# Chunk 3
belege <- scan(file.choose(), sep="\n", what=character(), fileEncoding="latin1")
# Chunk 4
belege <- scan(choose.files(), sep="\n", what=character(), fileEncoding="latin1")
# Chunk 5
h.ende <- grep("Kontext umschlie", belege)            # Bestimmt Ende des Headers der Exportdatei
belege <- belege[(h.ende+2):length(belege)]           # Lasse Header wegfallen
enden <- grep("(?<=/.{3}\\.[0-9]{5}).*\\)$", belege, perl=T)  # Bestimmt Einträge von beleg mit Quelle
# Chunk 6
out <- paste(belege[1:enden[1]], collapse=" ")
for(i in 2:(length(enden))){  									# korrigiert: i bis zur letzten Zeile laufen lassen
if (enden[i]-1==enden[i-1]){									# Falls ein Beleg in einer Zeile steht
out <- c(out, belege[enden[i]]) 							# fügt beleg in out hinzu
} else {														# Falls Beleg auf mehrere Zeilen verteilt ist
zus <- paste(belege[(enden[i-1]+1):enden[i]], collapse=" ") # Löscht den Zeilenumbruch; ersetzt ihn durch ein Leerzeichen " "
out <- c(out, zus)  										# Fügt beleg in out hinzu
}
}
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(here)
# Chunk 3
belege <- scan(file.choose(), sep="\n", what=character(), fileEncoding="latin1")
# Chunk 4
belege <- scan(choose.files(), sep="\n", what=character(), fileEncoding="latin1")
# Chunk 5
h.ende <- grep("Kontext umschlie", belege)            # Bestimmt Ende des Headers der Exportdatei
belege <- belege[(h.ende+2):length(belege)]           # Lasse Header wegfallen
enden <- grep("(?<=/.{3}\\.[0-9]{5}).*\\)$", belege, perl=T)  # Bestimmt Einträge von beleg mit Quelle
# Chunk 6
out <- paste(belege[1:enden[1]], collapse=" ")
for(i in 2:(length(enden))){  									# korrigiert: i bis zur letzten Zeile laufen lassen
if (enden[i]-1==enden[i-1]){									# Falls ein Beleg in einer Zeile steht
out <- c(out, belege[enden[i]]) 							# fügt beleg in out hinzu
} else {														# Falls Beleg auf mehrere Zeilen verteilt ist
zus <- paste(belege[(enden[i-1]+1):enden[i]], collapse=" ") # Löscht den Zeilenumbruch; ersetzt ihn durch ein Leerzeichen " "
out <- c(out, zus)  										# Fügt beleg in out hinzu
}
}
# Chunk 7
symb <- "§§§"
for (i in 1:length(out)){
auft <- unlist(strsplit(out[i], " "))							    # Macht aus out[i] den Wortvektor
pos.qu <- grep("^(\\(.{3,5})/", auft, perl=T)					# Bestimmt die Position der Quelle
pos.qu <- pos.qu[length(pos.qu)]								# Der letzte Ausdruck der Form (../.. wird als Quelle definiert
pos <- grep("<B>", auft[1:(pos.qu-1)])						    # Bestimmt die Position des Highlights
if (length(grep("\\.$", auft[1:pos], invert=TRUE))==pos) {	     # Falls kein Punkt vor dem Highlight/Vorkontext
pos1 <- 0
auft[pos1+1] <- paste(symb, auft[pos1+1])			  # Erzeugt ein Trennsymbol vor dem Satz mit Highlight
} else {													          	    # Falls Punkt existiert, also wenn Vorkontext existiert
pos1 <- grep("\\.$", auft[1:pos])						    # Sucht nach Punkten "." vor dem Satz mit Highlight.
pos1 <- pos1[length(pos1)]									    # pos1 = Position des Satzanfangs des Satzes mit Highlight.
auft[pos1] <- paste(auft[pos1], symb)						# Trennt Satz vor Highlight mit Satz mit Highlight
}
pos2 <- grep("\\.", auft[(pos1+1):(pos.qu-1)])				# Sucht nach Punkten "." nach dem Satz mit Highlight.
pos2 <- pos2[1]											                	# pos2 = Position des Satzendes des Satzes mit Highlight.
auft[(pos1+pos2)] <- paste(auft[(pos1+pos2)], symb)			# Trennt Satz nach Highlight mit Satz mit Highlight
auft <- c(auft[pos.qu:length(auft)], symb, auft[1:(pos.qu-1)], sep="") # Trennt Satz von Quelle und stellt beide um
out[i] <- paste(auft, collapse=" ")                  # Macht aus zerstückeltem Beleg einen Vektoreintrag in out
out[i] <- paste(paste(i, symb, sep=""), out[i])  # Füge Belegnummer vor der Quelle ein
auft <- unlist(strsplit(out[i], "§§§"))
if (length(auft) != 5){
auft <- c(auft, c(" "))
}
if (i == 1){
output <- matrix(auft, nrow = 1)  # Erstelle Matrixstruktur für ersten Beleg
} else {
output <- rbind(output, auft)     # Erstelle Matrixstruktur für Folgebelege
}
}
# Chunk 8
output <- as.data.frame(output)
colnames(output) <- c("Nr", "Quelle", "Vorkontext", "Treffer", "Nachkontext")
output
View(output)
shiny::runApp()
install.packages("tidyr")
install.packages(c("aplpack", "arm", "assertthat", "backports", "bayesplot", "BH", "bitops", "checkmate", "cli", "coda", "colorspace", "colourpicker", "crayon", "crosstalk", "curl", "data.table", "devtools", "digest", "doParallel", "DT", "e1071", "evaluate", "forcats", "foreach", "Formula", "ggplot2", "ggridges", "git2r", "gtable", "gtools", "haven", "highr", "Hmisc", "hms", "htmlTable", "htmltools", "htmlwidgets", "httpuv", "httr", "igraph", "inline", "iterators", "jsonlite", "knitr", "labeling", "languageR", "later", "lazyeval", "leaps", "lme4", "lmerTest", "lmtest", "loo", "lsmeans", "magrittr", "manipulateWidget", "maptools", "markdown", "matrixcalc", "MatrixModels", "matrixStats", "memoise", "mime", "mnormt", "multcomp", "nloptr", "numDeriv", "openssl", "openxlsx", "packrat", "pkgconfig", "PKI", "plyr", "polycor", "processx", "promises", "ps", "psych", "quantreg", "R6", "Rcpp", "RcppEigen", "RCurl", "readr", "readstata13", "readxl", "reshape2", "rgl", "rio", "RJSONIO", "rmarkdown", "rprojroot", "rsconnect", "rstan", "rstanarm", "rstantools", "rstudioapi", "sandwich", "scales", "shiny", "shinyjs", "shinythemes", "sp", "SparseM", "StanHeaders", "stringi", "stringr", "survey", "tables", "testthat", "TH.data", "threejs", "tinytex", "utf8", "viridis", "viridisLite", "webshot", "whisker", "withr", "xfun", "xtable", "xts", "yaml", "zip", "zoo"))
runApp()
shiny::runApp()
setwd("~/Documents/Work/A1/Corpus")
library(tidyr)
library(stringr)
library(dplyr)
library(readr)
library(purrr)
library(wordcloud2)
raw.file <- read_file("test.txt", locale(encoding="latin1"))
# Metadata ----------------------------------------------------------------
# Save COSMAS version
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Split file into sections
sections <- raw.file %>%
str_split("\\_{80}")
# Save export date
Export_Date <- sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Sentences and their information -----------------------------------------
corpora <- sections[[1]][3]
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(collapse = "|")
all_sentences <- sections[[1]][4]
text_parts <- all_sentences %>%
str_match_all(regex(paste("(.*?)<B>(.+?)</>(.*?)\\(((?:",corporaID,")/.*?)\\)\\s*\\n", sep=""),
dotall = TRUE))
# Source information
data <- data.frame(Sources = text_parts[[1]][,5])
# Tokens
data$Token <- text_parts[[1]][,3] %>%
str_trim()
unique_tokens <- unique(data$Token)
print(unique_tokens, row.names=FALSE)
# Context sentence BEFORE token sentence
data$Precontext <- text_parts[[1]][,2] %>%
str_extract_all(boundary("sentence")) %>%
map(function(x) {nth(x,-2)} ) %>%
str_trim() %>%
unlist()
# Sentence part BEFORE token
data$Prehit <- text_parts[[1]][,2] %>%
str_extract_all(boundary("sentence")) %>%
map(last) %>%
str_trim() %>%
unlist()
# Sentence part AFTER token
data$Posthit <- text_parts[[1]][,4] %>%
str_extract_all(boundary("sentence")) %>%
map(first) %>%
str_trim() %>%
unlist()
# Extract context sentence AFTER token sentence
data$Postcontext <- text_parts[[1]][,4] %>%
str_extract_all(boundary("sentence")) %>%
map(function(x) {nth(x,2)} ) %>%
str_trim() %>%
unlist()
corpora
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(collapse = "|")
all_sentences <- sections[[1]][4]
text_parts <- all_sentences %>%
str_match_all(regex(paste("(.*?)<B>(.+?)</>(.*?)\\(((?:",corporaID,")/.*?)\\)\\s*\\n", sep=""),
dotall = TRUE))
text_parts
runApp('COSMAS2-Reader')
setwd("~/Documents/Work/A1/Corpus")
library(tidyr)
library(stringr)
library(dplyr)
library(readr)
library(purrr)
library(wordcloud2)
# Reading in the raw COSMAS file
# FIXME  add encoding options "latin1" and "UTF-8"
# raw.file <- read_file("test.txt", locale(encoding="latin1"))
raw.file <- read_file("w_sentence_korpusansicht.TXT", locale(encoding="latin1"))
# Metadata ----------------------------------------------------------------
# Save COSMAS version
C2API_Version <- raw.file %>%
str_extract(regex("(?<=C2API-Version )(.*)(?= -)")) %>%
str_subset(regex(".*"))
# Split file into sections
sections <- raw.file %>%
str_split("\\_{80}")
# Save export date
Export_Date <- sections[[1]][2] %>%
str_extract(regex("(?<=\\n\\nDatum).+(?=\\nArchiv)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Save the search phrase
phrase <- sections[[1]][2] %>%
str_extract(regex("(?<=\\nSuchanfrage).+(?=\\nSuchoptionen)")) %>%
str_extract(regex("(?<=:\\s)(.*)$"))
# Sentences and their information -----------------------------------------
corpora <- sections[[1]][3]
corporaID <- corpora %>%
str_split("\\n") %>%
unlist() %>%
head(-7) %>%
str_extract(regex('(^\\w+)\\s(?=.*$)')) %>%
str_trim() %>%
str_subset(regex(".*")) %>%
unique() %>%
str_c(collapse = "|")
all_sentences <- sections[[1]][4]
all_sentences
all_sentences %>%
str_split(regex("^Korpus-Ansicht"))
a <- all_sentences %>%
str_split(regex("^Korpus-Ansicht"))
a[[1]]
a[[2]]
a <- all_sentences %>%
str_split(regex("Korpus-Ansicht"))
?str_split
a <- all_sentences %>%
str_split("Korpus-Ansicht")
a <- all_sentences %>%
str_split("Korpus-Ansicht, 925 Einträge,")
all_sentences %>%
str_extract("Korpus-Ansicht")
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht"))
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht\\,"))
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]"))
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]*"))
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]*Einträge"))
all_sentences %>%
str_extract(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]*\\s+Einträge"))
all_sentences
all_sentences %>%
str_split(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]*\\s+Einträge"))
all_sentences %>%
str_split(regex("\\nKorpus-Ansicht\\,\\s+[:digit:]*\\s+Einträge")) %>%
unlist()
shiny::runApp('COSMAS2-Reader')
runApp('COSMAS2-Reader')
